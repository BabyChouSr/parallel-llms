{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /data/chris/anaconda3/envs/peft-env/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/eecs/christopherchou/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "# Load eli5 dataset for the causal LM task\n",
    "# More info about Eli5: https://facebookresearch.github.io/ELI5/\n",
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2) # split dataset into 0.8-0.2\n",
    "eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"facebook/opt-1.3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '1s0bnc',\n",
       " 'title': 'How does trauma like from boxing or playing American football result in Tau protein build up? What about the physical impact results in changes in proteins produced?',\n",
       " 'selftext': '',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['cdsp3ee'],\n",
       " 'answers.text': [\"I'm assuming you're referring to the condition of chronic traumatic encephalopathy, or *dementia pugilistica* as it's commonly known as in the world of boxing.  This disease is seen in those with a history of head trauma, like football players and boxers, as you mentioned.  The disease itself shows symptoms of both alzheimer's dementia and parkinsonian dementia.  As you might expect [brains of these patients showed both tau protein (implicated in Alzhemier's) and alpha-synuclein (implicated in Parkinson's)](_URL_2_).\\n\\nTo get around to answering your question, it isn't totally clear!  [It is known that individuals with an APO E4 allele have increased risk of chronic traumatic encephalopathy](_URL_3_).  This allele (gene variant) is known to increases risk of alzheimer's in those that inherit it.  In those with other predisposing risks (repetitive head trauma), it also makes the resulting dementia that much worse.\\n\\nTo get a sense of the pathophysiology of the injury, picture the basic unit of the brain: [the neuron](_URL_0_).  Notice the long axon - these are susceptible to injury in head trauma - the axon ruptures, and communication in the brain is disrupted.  [Axons swell, rupture, and die](_URL_1_); also, various neurotransmitters and free radicals are released that cause secondary injury.\\n\\nIn other words, diffuse damage takes place across the brain, similar to Alzheimer's disease.  This manifests as the confusion and amnesia classic to Alzheimer's disease.  Again, I'm not totally clear from my own investigations into the literature how traumatic injuries result in a build-up of tau protein, whether there's even a causal link or if it's merely an additive phenomenon (meaning those with tau already accumulating in their brains manifest symptoms; those without it are more likely to not show symptoms).  \\n\\nMaybe someone with more of a neuroscience background can come in and add to this (very basic) coverage of CTE.\"],\n",
       " 'answers.score': [3],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': ['http://en.wikipedia.org/wiki/File:Neuron_Hand-tuned.svg',\n",
       "  'http://www.ncbi.nlm.nih.gov/pubmed/?term=15668572',\n",
       "  'http://www.ncbi.nlm.nih.gov/pubmed/23268705',\n",
       "  'http://www.ncbi.nlm.nih.gov/pubmed/?term=9214529']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5[\"train\"].flatten() # flatten because what we want is answers.text but it is nested in the answers' object\n",
    "eli5[0] # Notice how the answers.text is a list of strings that need to be joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    grouped_examples = [\" \".join(text) for text in examples[\"answers.text\"]] # converts list to a string that can be tokenized jointly\n",
    "    return tokenizer(grouped_examples, \n",
    "        padding=True,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_tensors=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.17ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function, \n",
    "    batched=True, # processes multiple elements of the dataset at once\n",
    "    remove_columns=eli5.column_names # remove these column names, or else we have column names + input_ids + attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since dataset has inputs that are of length that is greater than max_seq_length, we need to\n",
    "# 1. concatenate the sequences together\n",
    "# 2. truncate the sequences to max_seq_length\n",
    "# NOTE: this is only done for demonstration purposes, in reality, we should be dynamically padding our dataset\n",
    "def group_texts(examples):\n",
    "    concatenated_sequence = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_sequence[\"input_ids\"]) # length of the batch concatenated sequence that we are currently working with\n",
    "    total_length = (total_length // max_seq_length) * max_seq_length # truncate to a multiple of max_seq_length\n",
    "    result = {\n",
    "        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_sequence.items()\n",
    "    }\n",
    "    # result becomes\n",
    "    # \"input_id\" : [[64 tokens], [64 tokens], [64 tokens], ...]\n",
    "    # \"attention_mask\" : [[64 masks], [64 masks], [64 masks], ...]\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# tokenized_eli5 = tokenized_eli5.map(\n",
    "#     group_texts,\n",
    "#     batched=True,\n",
    "#     num_proc=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collators are used for creating batches out of our dataset and can also provide some preprocessing like dynamically padding\n",
    "data_collator = DataCollatorForLanguageModeling( # sets [-100] for pad tokens and the inputs for the labels\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False # masked language modelling set to false because we are doing causal language modelling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetune_opt_deepspeed_out\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer =  Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_eli5,\n",
    "    eval_dataset=tokenized_eli5,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 11.90 GiB total capacity; 11.24 GiB already allocated; 41.69 MiB free; 11.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:1634\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1631\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1633\u001b[0m )\n\u001b[0;32m-> 1634\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1635\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1636\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1637\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1638\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1639\u001b[0m )\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:1901\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1899\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1901\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1903\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1904\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1905\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1907\u001b[0m ):\n\u001b[1;32m   1908\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:2643\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2640\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2642\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2643\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2645\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2646\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:2675\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2674\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2675\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2676\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2677\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:172\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgather(outputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_device)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.gather\u001b[0;34m(self, outputs, output_device)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather\u001b[39m(\u001b[39mself\u001b[39m, outputs, output_device):\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m gather(outputs, output_device, dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:86\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(outputs, target_device, dim)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m# Recursive function calls like this create reference cycles.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m# Setting the function to None clears the refcycle.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     res \u001b[39m=\u001b[39m gather_map(outputs)\n\u001b[1;32m     87\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     gather_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:77\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m outputs):\n\u001b[1;32m     76\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAll dicts must have the same number of keys\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(out)((k, gather_map([d[k] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m outputs]))\n\u001b[1;32m     78\u001b[0m                      \u001b[39mfor\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m out)\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m<string>:8\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, loss, logits, past_key_values, hidden_states, attentions)\u001b[0m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/utils/generic.py:245\u001b[0m, in \u001b[0;36mModelOutput.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m# if we provided an iterator as first field and the iterator is a (key, value) iterator\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# set the associated fields\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m first_field_iterator:\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mfor\u001b[39;00m idx, element \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(iterator):\n\u001b[1;32m    246\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m             \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(element, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m))\n\u001b[1;32m    248\u001b[0m             \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(element) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(element[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m)\n\u001b[1;32m    250\u001b[0m         ):\n\u001b[1;32m    251\u001b[0m             \u001b[39mif\u001b[39;00m idx \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    252\u001b[0m                 \u001b[39m# If we do not have an iterator of key/values, set it as attribute\u001b[39;00m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:77\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m outputs):\n\u001b[1;32m     76\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAll dicts must have the same number of keys\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)((k, gather_map([d[k] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m outputs]))\n\u001b[1;32m     78\u001b[0m                      \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m out)\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:81\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(out)(\u001b[39mmap\u001b[39;49m(gather_map, \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49moutputs)))\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:81\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m _is_namedtuple(out):\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(out)\u001b[39m.\u001b[39m_make(\u001b[39mmap\u001b[39m(gather_map, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39moutputs)))\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(out)(\u001b[39mmap\u001b[39;49m(gather_map, \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49moutputs)))\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/scatter_gather.py:71\u001b[0m, in \u001b[0;36mgather.<locals>.gather_map\u001b[0;34m(outputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m out \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m Gather\u001b[39m.\u001b[39;49mapply(target_device, dim, \u001b[39m*\u001b[39;49moutputs)\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:75\u001b[0m, in \u001b[0;36mGather.forward\u001b[0;34m(ctx, target_device, dim, *inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     ctx\u001b[39m.\u001b[39munsqueezed_scalar \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     74\u001b[0m ctx\u001b[39m.\u001b[39minput_sizes \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(i\u001b[39m.\u001b[39msize(ctx\u001b[39m.\u001b[39mdim) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m comm\u001b[39m.\u001b[39;49mgather(inputs, ctx\u001b[39m.\u001b[39;49mdim, ctx\u001b[39m.\u001b[39;49mtarget_device)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/parallel/comm.py:235\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(tensors, dim, destination, out)\u001b[0m\n\u001b[1;32m    231\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    232\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mUsing -1 to represent CPU tensor is deprecated. Please use a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    233\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mdevice object or string instead, e.g., \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    234\u001b[0m     destination \u001b[39m=\u001b[39m _get_device_index(destination, allow_cpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_gather(tensors, dim, destination)\n\u001b[1;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m destination \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 11.90 GiB total capacity; 11.24 GiB already allocated; 41.69 MiB free; 11.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
