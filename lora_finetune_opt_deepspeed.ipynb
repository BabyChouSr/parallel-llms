{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/eecs/christopherchou/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "# Load eli5 dataset for the causal LM task\n",
    "# More info about Eli5: https://facebookresearch.github.io/ELI5/\n",
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/eecs/christopherchou/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-34e7178cabffffe7.arrow and /home/eecs/christopherchou/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-3459f37d3c7812d8.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2) # split dataset into 0.8-0.2\n",
    "eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"facebook/opt-1.3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '14p632',\n",
       " 'title': 'Can someone explain why this is a valid method of measuring the obesity rate?',\n",
       " 'selftext': '',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers.a_id': ['c7f5e1w'],\n",
       " 'answers.text': [\"The article is using information directly from the CDC: [Prevalence of Obesity Among Children and Adolescents: United States, Trends 1963-1965 Through 2007-2008](_URL_1_). As far as I can tell, the cutoff for obesity was fixed to the 2000 charts, so it makes sense to compare obesity rates across different years. Those charts are based on data gathered from 1963-1994. (You can find the charts and methods [here](_URL_0_).) Notice that it hits around the exact 5% obesity (which you'd expect using the 95th percentile) in the '70s.\"],\n",
       " 'answers.score': [3],\n",
       " 'title_urls.url': [],\n",
       " 'selftext_urls.url': [],\n",
       " 'answers_urls.url': ['http://www.cdc.gov/growthcharts/',\n",
       "  'http://www.cdc.gov/nchs/data/hestat/obesity_child_07_08/obesity_child_07_08.htm']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5[\"train\"].flatten() # flatten because what we want is answers.text but it is nested in the answers' object\n",
    "eli5[0] # Notice how the answers.text is a list of strings that need to be joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    grouped_examples = [\" \".join(text) for text in examples[\"answers.text\"]] # converts list to a string that can be tokenized jointly\n",
    "    return tokenizer(grouped_examples, \n",
    "        padding=True,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        return_tensors=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/eecs/christopherchou/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-885b2b9230497ec9.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function, \n",
    "    batched=True, # processes multiple elements of the dataset at once\n",
    "    remove_columns=eli5.column_names # remove these column names, or else we have column names + input_ids + attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since dataset has inputs that are of length that is greater than max_seq_length, we need to\n",
    "# 1. concatenate the sequences together\n",
    "# 2. truncate the sequences to max_seq_length\n",
    "# NOTE: this is only done for demonstration purposes, in reality, we should be dynamically padding our dataset\n",
    "# def group_texts(examples):\n",
    "#     concatenated_sequence = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_sequence[\"input_ids\"]) # length of the batch concatenated sequence that we are currently working with\n",
    "#     total_length = (total_length // max_seq_length) * max_seq_length # truncate to a multiple of max_seq_length\n",
    "#     result = {\n",
    "#         k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "#         for k, t in concatenated_sequence.items()\n",
    "#     }\n",
    "#     # result becomes\n",
    "#     # \"input_id\" : [[64 tokens], [64 tokens], [64 tokens], ...]\n",
    "#     # \"attention_mask\" : [[64 masks], [64 masks], [64 masks], ...]\n",
    "\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result\n",
    "\n",
    "# tokenized_eli5 = tokenized_eli5.map(\n",
    "#     group_texts,\n",
    "#     batched=True,\n",
    "#     num_proc=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collators are used for creating batches out of our dataset and can also provide some preprocessing like dynamically padding\n",
    "data_collator = DataCollatorForLanguageModeling( # sets [-100] for pad tokens and the inputs for the labels\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False # masked language modelling set to false because we are doing causal language modelling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 1317330944 || trainable%: 0.11939778740975206\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model.enable_input_require_grads() # fixes issue of RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 8\n",
    "per_device_batch_size = 1\n",
    "\n",
    "assert global_batch_size % per_device_batch_size == 0, \"global_batch_size must be divisible by per_device_batch_size\"\n",
    "gradient_accumulation_steps = global_batch_size // per_device_batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetune_opt_deepspeed_out\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    deepspeed=\"ds_config.json\", # use deepspeed for CPU offloading of optimizers, gradients, and parameters (3)\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# OOM without deepspeed NOTE: have to run deepspeed on .py file instead of .ipynb since need > 1 process\n",
    "trainer =  Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_eli5,\n",
    "    eval_dataset=tokenized_eli5,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-05 15:33:11,999] [WARNING] [engine.py:1223:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Using /home/eecs/christopherchou/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load utils op: 0.0036559104919433594 seconds\n",
      "Parameter Offload: Total persistent parameters: 2215936 in 338 params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/eecs/christopherchou/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load utils op: 0.0009481906890869141 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-05 15:33:29,224] [WARNING] [stage3.py:1939:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding torch.cuda.empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/1500 00:10 < 4:12:56, 0.10 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:1634\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1631\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1633\u001b[0m )\n\u001b[0;32m-> 1634\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1635\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1636\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1637\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1638\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1639\u001b[0m )\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:1901\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1899\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1901\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1903\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1904\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1905\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1907\u001b[0m ):\n\u001b[1;32m   1908\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:2659\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2656\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2657\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[1;32m   2658\u001b[0m     \u001b[39m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[39;00m\n\u001b[0;32m-> 2659\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeepspeed\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2660\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2661\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/utils/nvtx.py:11\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     10\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mnvtx\u001b[39m.\u001b[39mrange(func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/runtime/engine.py:1969\u001b[0m, in \u001b[0;36mDeepSpeedEngine.backward\u001b[0;34m(self, loss, allreduce_gradients, release_loss, retain_graph, scale_wrt_gas)\u001b[0m\n\u001b[1;32m   1966\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_optimization():\n\u001b[1;32m   1967\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mis_gradient_accumulation_boundary \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_gradient_accumulation_boundary(\n\u001b[1;32m   1968\u001b[0m     )\n\u001b[0;32m-> 1969\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mbackward(loss, retain_graph\u001b[39m=\u001b[39;49mretain_graph)\n\u001b[1;32m   1970\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp_enabled():\n\u001b[1;32m   1971\u001b[0m     \u001b[39m# AMP requires delaying unscale when inside gradient accumulation boundaries\u001b[39;00m\n\u001b[1;32m   1972\u001b[0m     \u001b[39m# https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\u001b[39;00m\n\u001b[1;32m   1973\u001b[0m     delay_unscale \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_gradient_accumulation_boundary()\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/utils/nvtx.py:11\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     10\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mnvtx\u001b[39m.\u001b[39mrange(func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py:2083\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer_Stage3.backward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2082\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2083\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_scaler\u001b[39m.\u001b[39;49mbackward(loss\u001b[39m.\u001b[39;49mfloat(), retain_graph\u001b[39m=\u001b[39;49mretain_graph)\n\u001b[1;32m   2085\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_coordinator(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mreset_step()\n\u001b[1;32m   2087\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mswap_optimizer:\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py:51\u001b[0m, in \u001b[0;36mLossScalerBase.backward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, loss, retain_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     scaled_loss \u001b[39m=\u001b[39m loss \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_scale\n\u001b[0;32m---> 51\u001b[0m     scaled_loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49mretain_graph)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 105M total params, 102M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "    2.64GB |   0.38GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "    2.64GB |   0.38GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "    2.35GB |   0.58GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "    2.35GB |   0.58GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    0.58GB |   2.15GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "    0.59GB |   2.15GB | offload_param=none, offload_optimizer=none, zero_init=0\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live\n",
    "\n",
    "# 6.4 GB on GPU required for micro_batch_size of 1... \n",
    "estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
