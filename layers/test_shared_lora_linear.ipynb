{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import LoraLinear, SharedLoraLinear\n",
    "from peft import PeftConfig, PeftModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "# opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = opt_model.state_dict()\n",
    "# model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model_name_or_path': 'facebook/opt-125m',\n",
       " 'bias': 'none',\n",
       " 'fan_in_fan_out': False,\n",
       " 'inference_mode': True,\n",
       " 'init_lora_weights': True,\n",
       " 'layers_pattern': None,\n",
       " 'layers_to_transform': None,\n",
       " 'lora_alpha': 16,\n",
       " 'lora_dropout': 0.05,\n",
       " 'modules_to_save': None,\n",
       " 'peft_type': 'LORA',\n",
       " 'r': 8,\n",
       " 'revision': None,\n",
       " 'target_modules': ['q_proj', 'v_proj'],\n",
       " 'task_type': 'CAUSAL_LM'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_config = json.load(open('/data/chris/adapters/peft-opt125m-dummylora/adapter_config.json'))\n",
    "opt_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tensor for the \n",
    "weight = model_state_dict['model.decoder.layers.0.self_attn.v_proj.weight']\n",
    "bias = model_state_dict['model.decoder.layers.0.self_attn.v_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_B.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_A.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_B.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_A.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_B.weight'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_weights_one = torch.load('/data/chris/adapters/peft-opt125m-dummylora/adapter_model.bin')\n",
    "adapter_weights_one.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_A = adapter_weights_one[\"base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.weight\"]\n",
    "lora_B = adapter_weights_one[\"base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_weights_two = torch.load('/data/chris/adapters/peft-opt125m-dummylora2/adapter_model.bin')\n",
    "# adapter_weights_two.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_A_two = adapter_weights_two[\"base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.weight\"]\n",
    "lora_B_two= adapter_weights_two[\"base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 768\n",
    "out_features = 8\n",
    "linear = SharedLoraLinear(in_features, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.update_layer(\"dummy-lora-one\", 8, 16, 0.0)\n",
    "linear.update_layer(\"dummy-lora-two\", 8, 16, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight = nn.Parameter(weight)\n",
    "linear.bias = nn.Parameter(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (dummy-lora-one): LoraLinear(\n",
       "    in_features=768, out_features=8, bias=True\n",
       "    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "    (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "  )\n",
       "  (dummy-lora-two): LoraLinear(\n",
       "    in_features=768, out_features=8, bias=True\n",
       "    (lora_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "    (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.loras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.loras[\"dummy-lora-one\"].lora_A.weight = nn.Parameter(lora_A.float())\n",
    "linear.loras[\"dummy-lora-one\"].lora_B.weight = nn.Parameter(lora_B.float())\n",
    "linear.loras[\"dummy-lora-two\"].lora_A.weight = nn.Parameter(lora_A_two.float())\n",
    "linear.loras[\"dummy-lora-two\"].lora_B.weight = nn.Parameter(lora_B_two.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input_one = torch.rand(1, 768).float()\n",
    "model_input_two = torch.rand(1, 768).float()\n",
    "# concat queries into one tensor\n",
    "\n",
    "model_input = torch.cat((model_input_one, model_input_two), dim=0)\n",
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.7 ms, sys: 0 ns, total: 74.7 ms\n",
      "Wall time: 5.03 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "result = linear.forward(model_input, [\"dummy-lora-one\", \"dummy-lora-two\"])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 236 ms, sys: 12.7 ms, total: 249 ms\n",
      "Wall time: 21.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4346e-01,  1.4589e-04,  1.4613e-01,  ..., -1.1955e-01,\n",
       "         -1.6227e-01,  8.0859e-02],\n",
       "        [-2.9381e-03, -3.8881e-02, -9.4747e-02,  ...,  8.7724e-03,\n",
       "         -1.6797e-01,  1.8147e-01]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scaling = 8/ 16\n",
    "manual_result = F.linear(model_input_one, linear.weight, linear.bias)\n",
    "manual_result += (linear.loras[\"dummy-lora-one\"].lora_B((linear.loras[\"dummy-lora-one\"].lora_A(model_input_one)))) * scaling\n",
    "\n",
    "manual_result_two = F.linear(model_input_two, linear.weight, linear.bias)\n",
    "manual_result_two += (linear.loras[\"dummy-lora-two\"].lora_B((linear.loras[\"dummy-lora-two\"].lora_A(model_input_two)))) * scaling\n",
    "\n",
    "# concat manual result and manual result two\n",
    "manual_final_result = torch.cat((manual_result, manual_result_two), dim=0)\n",
    "manual_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(result, manual_final_result, atol=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 ms, sys: 5.05 ms, total: 178 ms\n",
      "Wall time: 24.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "model_inputs = torch.rand(100, 1, 768).float()\n",
    "adapter_names = [\"dummy-lora-one\", \"dummy-lora-two\"] * 50\n",
    "result = linear.forward(model_inputs, adapter_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.47 s, sys: 159 ms, total: 8.63 s\n",
      "Wall time: 609 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "manual_final_results = torch.zeros(100, 1, 768)\n",
    "for i in range(100):\n",
    "    result_i = F.linear(model_inputs[i], linear.weight, linear.bias) + (linear.loras[adapter_names[i]].lora_B((linear.loras[adapter_names[i]].lora_A(model_inputs[i])))) * scaling       \n",
    "    manual_final_results[i] = result_i\n",
    "\n",
    "torch.allclose(result, manual_final_results, atol=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
