{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /data/chris/anaconda3/envs/peft-env/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import AdaptionPromptConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from utils.prompter import Prompter\n",
    "from utils.make_dataset import DatasetCreator\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "# model_name = \"facebook/opt-1.3b\"\n",
    "adapter_len = 10 \n",
    "adapter_layers = 30\n",
    "cutoff_len = 256\n",
    "train_on_inputs = True\n",
    "add_eos_token = False\n",
    "batch_size: int = 1\n",
    "micro_batch_size: int = 1\n",
    "num_epochs: int = 3\n",
    "learning_rate: float = 3e-4\n",
    "cutoff_len: int = 256\n",
    "val_set_size: int = 2000\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.64s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = AdaptionPromptConfig(adapter_layers=1, adapter_len=4, task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16385 || all params: 6738432001 || trainable%: 0.00024315745855368765\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e02b8a396fbb2bb4\n",
      "Found cached dataset json (/home/eecs/christopherchou/.cache/huggingface/datasets/json/default-e02b8a396fbb2bb4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 1/1 [00:00<00:00, 245.80it/s]\n",
      "Loading cached split indices for dataset at /home/eecs/christopherchou/.cache/huggingface/datasets/json/default-e02b8a396fbb2bb4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c4a703fef6e6f08c.arrow and /home/eecs/christopherchou/.cache/huggingface/datasets/json/default-e02b8a396fbb2bb4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fcabdebf1e613dd6.arrow\n",
      "Loading cached shuffled indices for dataset at /home/eecs/christopherchou/.cache/huggingface/datasets/json/default-e02b8a396fbb2bb4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c6a37338a965bb27.arrow\n",
      "100%|██████████| 46801/46801 [01:16<00:00, 608.92ex/s]\n",
      "100%|██████████| 5201/5201 [00:08<00:00, 610.56ex/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"alpaca_data_gpt4.json\"\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "\n",
    "val_set_size = 0.1\n",
    "dataset_creator = DatasetCreator(\n",
    "    tokenizer=tokenizer,\n",
    "    prompter=Prompter(template_name=\"alpaca\"),\n",
    "    cutoff_len=cutoff_len,\n",
    "    train_on_inputs=train_on_inputs,\n",
    "    add_eos_token=add_eos_token,\n",
    "    val_set_size=val_set_size,\n",
    ")\n",
    "\n",
    "dataset = dataset_creator.create_dataset(data)\n",
    "train_set = dataset[\"train\"]\n",
    "val_set = dataset[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./llama-adapter-7b\",\n",
    "    learning_rate=learning_rate,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    save_steps=3,\n",
    "    deepspeed=\"ds_config.json\",\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 11.90 GiB total capacity; 10.84 GiB already allocated; 41.69 MiB free; 11.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:1634\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1631\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1633\u001b[0m )\n\u001b[0;32m-> 1634\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1635\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1636\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1637\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1638\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1639\u001b[0m )\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/trainer.py:1703\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1696\u001b[0m delay_optimizer_creation \u001b[39m=\u001b[39m (\n\u001b[1;32m   1697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msharded_ddp \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msharded_ddp \u001b[39m!=\u001b[39m ShardedDDPOption\u001b[39m.\u001b[39mSIMPLE\n\u001b[1;32m   1699\u001b[0m     \u001b[39mor\u001b[39;00m is_sagemaker_mp_enabled()\n\u001b[1;32m   1700\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfsdp \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1701\u001b[0m )\n\u001b[1;32m   1702\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdeepspeed:\n\u001b[0;32m-> 1703\u001b[0m     deepspeed_engine, optimizer, lr_scheduler \u001b[39m=\u001b[39m deepspeed_init(\n\u001b[1;32m   1704\u001b[0m         \u001b[39mself\u001b[39;49m, num_training_steps\u001b[39m=\u001b[39;49mmax_steps, resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint\n\u001b[1;32m   1705\u001b[0m     )\n\u001b[1;32m   1706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m deepspeed_engine\u001b[39m.\u001b[39mmodule\n\u001b[1;32m   1707\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m deepspeed_engine\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/transformers/deepspeed.py:378\u001b[0m, in \u001b[0;36mdeepspeed_init\u001b[0;34m(trainer, num_training_steps, resume_from_checkpoint, inference)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39m# keep for quick debug:\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m# from pprint import pprint; pprint(config)\u001b[39;00m\n\u001b[1;32m    370\u001b[0m kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    371\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m    372\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel_parameters\u001b[39m\u001b[39m\"\u001b[39m: model_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlr_scheduler\u001b[39m\u001b[39m\"\u001b[39m: lr_scheduler,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m deepspeed_engine, optimizer, _, lr_scheduler \u001b[39m=\u001b[39m deepspeed\u001b[39m.\u001b[39;49minitialize(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m resume_from_checkpoint \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[39m# it's possible that the user is trying to resume from model_path, which doesn't necessarily\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[39m# contain a deepspeed checkpoint. e.g. examples just check if the dir exists and assume it's\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39m# a resume from a checkpoint and not just a local pretrained weight. So we check here if the\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# path contains what looks like a deepspeed checkpoint\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/__init__.py:125\u001b[0m, in \u001b[0;36minitialize\u001b[0;34m(args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config, config_params)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39massert\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mdeepspeed.initialize requires a model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model, PipelineModule):\n\u001b[0;32m--> 125\u001b[0m     engine \u001b[39m=\u001b[39m DeepSpeedEngine(args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    126\u001b[0m                              model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    127\u001b[0m                              optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    128\u001b[0m                              model_parameters\u001b[39m=\u001b[39;49mmodel_parameters,\n\u001b[1;32m    129\u001b[0m                              training_data\u001b[39m=\u001b[39;49mtraining_data,\n\u001b[1;32m    130\u001b[0m                              lr_scheduler\u001b[39m=\u001b[39;49mlr_scheduler,\n\u001b[1;32m    131\u001b[0m                              mpu\u001b[39m=\u001b[39;49mmpu,\n\u001b[1;32m    132\u001b[0m                              dist_init_required\u001b[39m=\u001b[39;49mdist_init_required,\n\u001b[1;32m    133\u001b[0m                              collate_fn\u001b[39m=\u001b[39;49mcollate_fn,\n\u001b[1;32m    134\u001b[0m                              config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    135\u001b[0m                              config_params\u001b[39m=\u001b[39;49mconfig_params)\n\u001b[1;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39massert\u001b[39;00m mpu \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mmpu must be None with pipeline parallelism\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/runtime/engine.py:301\u001b[0m, in \u001b[0;36mDeepSpeedEngine.__init__\u001b[0;34m(self, args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config, config_params, dont_change_device)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline_parallelism \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(model, PipelineModule)\n\u001b[1;32m    300\u001b[0m \u001b[39m# Configure distributed model\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_configure_distributed_model(model)\n\u001b[1;32m    303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_model_parameters()\n\u001b[1;32m    305\u001b[0m see_memory_usage(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDeepSpeed Engine: After configure distributed model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/deepspeed/runtime/engine.py:1151\u001b[0m, in \u001b[0;36mDeepSpeedEngine._configure_distributed_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__check_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, torch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m   1150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdont_change_device:\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m   1153\u001b[0m \u001b[39m# MoE related initialization\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[39mfor\u001b[39;00m _, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m/data/chris/anaconda3/envs/peft-env/lib/python3.8/site-packages/bitsandbytes/nn/modules.py:199\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcuda(device)\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     new_param \u001b[39m=\u001b[39m Int8Params(\n\u001b[0;32m--> 199\u001b[0m         \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    200\u001b[0m             device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mdtype, non_blocking\u001b[39m=\u001b[39;49mnon_blocking\n\u001b[1;32m    201\u001b[0m         ),\n\u001b[1;32m    202\u001b[0m         requires_grad\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad,\n\u001b[1;32m    203\u001b[0m         has_fp16_weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_fp16_weights,\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    205\u001b[0m     new_param\u001b[39m.\u001b[39mCB \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCB\n\u001b[1;32m    206\u001b[0m     new_param\u001b[39m.\u001b[39mSCB \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSCB\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 11.90 GiB total capacity; 10.84 GiB already allocated; 41.69 MiB free; 11.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
